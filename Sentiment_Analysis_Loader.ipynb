{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_Loader.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29sK1OH1CLDJ"
      },
      "outputs": [],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import io\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, precision_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from emoji import emojize\n",
        "import emoji\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import re,string, nltk\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK35BBITCYfh",
        "outputId": "4617aaf0-1206-4bb3-c182-889fb3910853"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = pickle.load(open(\"/content/sample_data/vectorizer.pickle\", \"rb\"))\n",
        "model = pickle.load(open(\"/content/sample_data/model.pickle\", \"rb\"))"
      ],
      "metadata": {
        "id": "u7qJU1AdCcXA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = ['Bro. U gotta chill RT @CHILLShrammy: Dog FUCK KP that dumb nigger bitch lmao']"
      ],
      "metadata": {
        "id": "S9ObNmGQCdHe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0] = x[0].replace(r\"http\\S+\",\" \")\n",
        "x[0]  = x[0].replace(r\"http\",\" \")\n",
        "x[0]  = x[0].replace(r\"@\",\"at\")\n",
        "x[0]  = x[0].replace(\"#[A-Za-z0-9_]+\", ' ')\n",
        "x[0]  = x[0].replace(r\"[^A-Za-z(),!?@\\'\\\"_\\n]\",\" \")\n",
        "x[0]  = x[0].lower()"
      ],
      "metadata": {
        "id": "p_5hHrvHChEB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "saEKSr9SCk5H",
        "outputId": "5df4beb2-5299-4a5c-fde0-94ba160d9ab7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bro. u gotta chill rt atchillshrammy: dog fuck kp that dumb nigger bitch lmao'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Lemmmatizer to remove tenses from texts.\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "STOPWORDS.update(['rt', 'mkr', 'didn', 'bc', 'n', 'm', \n",
        "                  'im', 'll', 'y', 've', 'u', 'ur', 'don', \n",
        "                  'p', 't', 's', 'aren', 'kp', 'o', 'kat', \n",
        "                  'de', 're', 'amp', 'will'])\n",
        "corpus = []\n",
        "def preprocess_tweet(tweet):\n",
        "    tweet = re.sub(r\"won\\'t\", \"will not\", tweet)\n",
        "    tweet = re.sub(r\"can\\'t\", \"can not\", tweet)\n",
        "    tweet = re.sub(r\"n\\'t\", \" not\", tweet)\n",
        "    tweet = re.sub(r\"\\'re\", \" are\", tweet)\n",
        "    tweet = re.sub(r\"\\'s\", \" is\", tweet)\n",
        "    tweet = re.sub(r\"\\'d\", \" would\",tweet)\n",
        "    tweet = re.sub(r\"\\'ll\", \" will\", tweet)\n",
        "    tweet = re.sub(r\"\\'t\", \" not\", tweet)\n",
        "    tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n",
        "    tweet = re.sub(r\"\\'m\", \" am\", tweet)\n",
        "    tweet = re.sub('[^a-zA-Z]',' ',tweet)\n",
        "    tweet = re.sub(emoji.get_emoji_regexp(),\"\",tweet)\n",
        "    tweet = re.sub(r'[^\\x00-\\x7f]','',tweet)\n",
        "    tweet = \" \".join([stemmer.stem(word) for word in tweet.split()])\n",
        "    tweet = [lemmatizer.lemmatize(word) for word in tweet.split() if not word in set(STOPWORDS)]\n",
        "    tweet = ' '.join(tweet)\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "oDmuzf2FCnyq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new = tfidf.transform([x[0]])"
      ],
      "metadata": {
        "id": "FLk-v9J8CvNA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict = model.predict(new)"
      ],
      "metadata": {
        "id": "xStPQ0EcMhrw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = predict\n",
        "lc = l[0]\n",
        "print('Category',*l)\n",
        "\n",
        "labels = {\"not_cyberbullying\":0,\"gender\":1,\"ethnicity\":2,\"religion\":3,\"age\":4}\n",
        "\n",
        "for i in labels:\n",
        "  if (labels[i] == l[0]):\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej8wBLwXCwdM",
        "outputId": "7235f06e-4be0-47f3-93a4-c7098bc64e1c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category 2\n",
            "ethnicity\n"
          ]
        }
      ]
    }
  ]
}